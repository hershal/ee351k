\documentclass[12pt]{article}
\title{EE351K Homework 10}
\author{Hershal Bhave (hb6279)}
\date{Due November 20, 2014}

\usepackage{multicol}
\usepackage{float}
\usepackage[in]{fullpage}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{tabularx}
\usepackage{rotating}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{cleveref}
\usepackage{graphics}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[nosolutionfiles]{answers}
\usepackage[acronym]{glossaries}

\newenvironment{Ex}{\textbf{Problem}\vspace{.75em}\\}{}
\Newassociation{solution}{Soln}{Answers}
\pagebreak[3]
\newcommand{\Opentesthook}[2]{\Writetofile{#1}{\protect\section{#1: #2}}}
\renewcommand{\Solnlabel}[1]{\textbf{Solution}\quad}

\newcommand{\dd}[1]{\:\mathrm{d}{#1}}
\newcommand{\ddt}[1]{\frac{\dd{}}{\dd{#1}}}
\newcommand{\dddt}[1]{\frac{\dd{}^2}{\dd{#1}^2}}

\DeclareMathOperator*{\argmax}{arg\,max}

\definecolor{silver}{rgb}{0.95,0.95,0.95}

\begin{document}
\maketitle
\begin{enumerate}
\item
  \begin{Ex}
    At a computer disk drive factory, inspectors randomly pick a
    product from production lines to detect a failure. If the
    production lines are normal, this failure rate $q_0 =
    10^{-4}$. However occasionally some problems occur in the lines,
    in which case the rate goes up to $q_1 = 10^{-1}$. Let $H_i$
    denote the hypothesis that the failure rate is $q_i$.

    Every morning, an inspector chooses drives at random from the
    previous day’s production and tests them. If a failure occurs too
    soon, the company stops production and checks the critical part of
    the process. Production line problems occur at random once every
    10 days, so that $P(H_1) = 0.1 = 1 - P(H_0)$.

    \begin{enumerate}
    \item Based on $N$, the number of drives tested up to and
      including the first failure, design a MAP test.
    \item Calculate the probability of ``false alarm'' and the
      probability of ``missed detection''.
    \item Based on this, calculate the probability of detection error
      $P_e$.
    \end{enumerate}
    \begin{solution} \hfill
      \begin{enumerate}
      \item First we will set up the problem. $P(H_0)=p=0.9$ and
        $P(H_1)=1-p=0.1$ so that
        \begin{equation}
          \label{eq:1a-n-dist}
          N\sim\left\{
            \begin{aligned}
              & \text{Geometric}(q_0) && \quad\text{w.p. } p \\
              & \text{Geometric}(q_1) && \quad\text{w.p. } 1-p \\
            \end{aligned} \right.
        \end{equation}
        We will define 0 and 1 to map to $H_0$ and $H_1$
        respectively so that
        \begin{equation}
          \label{eq:1a-h-mapping}
          \begin{aligned}
            P_\Theta(0) = 0.9 \\
            P_\Theta(1) = 0.1 \\
          \end{aligned}
        \end{equation}
        For this problem, we would like to find $\hat{\theta}$ which
        maximizes $P_{\Theta|N}(\theta|n)$.
        \begin{equation}
          \label{eq:1a-theta-hat}
          \hat{\theta}_{\text{MAP}}(n) = \argmax_{\theta \in \{0,1\}}
          P_{\Theta|N}(\theta|n)
        \end{equation}
        Which becomes
        \begin{equation}
          \label{eq:1a-theta-hat-expanded}
          \hat{\theta}_{\text{MAP}} = \left\{
            \begin{aligned}
              & 0 &&\quad n>n^* \\
              & 1 &&\quad n\le n^* \\
            \end{aligned} \right.
        \end{equation}
        So that $H_0$ is chosen if $n>n^*$ and $H_1$ is chosen if
        $n\le n^*$.

        \begin{mdframed}[backgroundcolor=silver]
          \begin{description}
          \item[MAP Rule] \hfill \\
            $$\hat{\theta}_{\text{MAP}}(n) = \argmax_{\theta}
            P_{\Theta|N}(\theta|n)$$
            Equivalently, it selects $\hat{\theta}$ that maximizes
            over $\theta$
            $$f_{\Theta}(\theta)f_{X|\Theta}(x|\theta)$$
            So that if $\theta\in\{0,1\}$, then the corresponding MAP
            test to decide in favor of $\theta=1$ looks like
            $$f_{\Theta}(0)f_{X|\Theta}(x|0) >
            f_{\Theta}(1)f_{X|\Theta}(x|1)$$
          \end{description}
        \end{mdframed}

        The corresponding MAP test is in the form
        \begin{equation}
          \label{eq:1a-map-test}
          \begin{aligned}
            p_{\Theta}(q_0)p_{N|\Theta}(n|q_0)
            &> p_{\Theta}(q_1)p_{N|\Theta}(n|q_1) \\
            P(H_0)p_{N|\Theta}(n|q_0)
            &> P(H_1)p_{N|\Theta}(n|q_1) \\
            \frac{P(H_0)}{P(H_1)}
            &> \frac{p_{N|\Theta}(n|q_1)}{p_{N|\Theta}(n|q_0)} \\
            \frac{P(H_0)}{P(H_1)}
            &> \frac{(1-q_1)^{n-1}(q_1)}{(1-q_0)^{n-1}(q_0)} \\
            \frac{P(H_0)(q_0)}{P(H_1)(q_1)}
            &> \frac{(1-q_1)^{n-1}}{(1-q_0)^{n-1}} \\
            \log\left(\frac{P(H_0)(q_0)}{P(H_1)(q_1)}\right)
            &> \log[(1-q_1)^{n-1}] - \log[(1-q_0)^{n-1}] \\
            \log\left(\frac{P(H_0)(q_0)}{P(H_1)(q_1)}\right)
            &> (n-1)\left(\frac{\log(1-q_1)}{\log(1-q_0)}\right) \\
            n-1 &<
            \frac{\log\left(\frac{P(H_0)(q_0)}{P(H_1)(q_1)}\right)}
            {\left(\frac{\log(1-q_1)}{\log(1-q_0)}\right)} \\
            \implies n^* &= 1 +
            \frac{\log\left(\frac{P(H_0)(q_0)}{P(H_1)(q_1)}\right)}
            {\left(\frac{\log(1-q_1)}{\log(1-q_0)}\right)} \\
            \implies n^* &= 45.7512 \\
          \end{aligned}
        \end{equation}

        This means that after 46 drives are tested, we can make a
        decision about which state of production quality the factory
        is in. If a failure occurs before $n=46$, then we can assume
        that $H_1$ is true and the failure rate of the factory is is
        $q_1$. If the worker tests 46 drives and no failures occur,
        then we can assume that the $H_0$ is true and the failure rate
        of the factory is $q_0$.
      \item The probability of ``false alarm'' is the probability that
        the number of drives until a failure is less than 46 given
        that the factory's production is normal (i.e. $H_0$ is
        true). The probability of ``false alarm'' can be found by the
        following
        \begin{equation}
          \label{eq:1b-false-alarm}
          \begin{aligned}
            P(N\le n^*|H_0) &= \int_1^{n^*} (1-q_0)^{n^*-1}(q_0)\dd{n^*} \\
            P(N\le 46|H_0) &=\int_1^{46} (1-q_0)^{n^*}(q_0)\dd{n^*} \\
            \implies P(N\le 46|H_0) &= 0.00449011 \\
          \end{aligned}
        \end{equation}
        The probability of a ``false alarm'' is less than 0.5\%.

        The probability of a ``missed detection'' is the probability
        that the factory the number of drives untail a failure is
        greater than 46 given that the factory's production is
        abnormal. The probability of a ``missed detection'' can be
        found by the following

        \begin{equation}
          \label{eq:1b-missed-detection}
          \begin{aligned}
            P(N\ge n^*|H_1) &= \int_{n^*}^{\infty}
            (1-q_1)^{n^*-1}(q_1)\dd{n^*} \\
            P(N\ge 46|H_1) &= \int_{46}^{\infty}
            (1-q_1)^{n^*-1}(q_1)\dd{n^*} \\
            \implies P(N\ge 46|H_1) &= 0.00872796 \\
          \end{aligned}
        \end{equation}
        The probability of a ``missed detection'' is less than 0.9\%.
      \item The probabilty of a ``detection error'' is the probability
        that the factory is in the $H_0$ state and there is a false
        alarm or the factory is in the $H_1$ state and there is a
        missed detection.

        % (format "%f" (+ (* 0.9 0.00449011) (* 0.1 0.00872796)))
        \begin{equation}
          \label{eq:1c-detection-error}
          \begin{aligned}
            P_e &= P(H_0)P(N\le n^*|H_0) +  P(H_1)P(N\le n^*|H_1) \\
            &= (0.9)(0.00458967) + (0.1)(0.00872796) \\
            \implies P_e &= 0.0049139 \\
          \end{aligned}
        \end{equation}
      \end{enumerate}
    \end{solution}
  \end{Ex}
\pagebreak[4]
\item
  \begin{Ex}
    Suppose that $X_1,\ldots,X_n$ form a random sample from a uniform
    distribution on the interval $(0,\theta)$, where of the parameter
    $\theta > 0$ but is unknown. Please find MLE of $\theta$.
    \begin{solution} \hfill
      \\ {\huge \color{red} MLE==Maximum Likelihood Estimation?}
    \end{solution}
  \end{Ex}
\item
  \begin{Ex}
    Let $X$ be a random variable uniform over the interval $[2,
    12]$. Suppose we observe $X$ with some random error $N$ which is
    uniformly distributed over interval $[0,1]$. i.e., $Y=X+N$
    \begin{enumerate}
    \item Find the Least mean square estimate of X based on the
      observation $Y = y$.
    \item Find the mean square error.
    \end{enumerate}
    \begin{solution} \hfill
      \begin{enumerate}
      \item Before we attempt this problem we must analyze the data we
        are given. We know:
        \begin{equation}
          \label{eq:3a-defs}
          \begin{aligned}
            X&\sim\text{Uniform}(2,12) \\
            N&\sim\text{Uniform}(0,1) \\
            Y&=X+N \\
          \end{aligned}
        \end{equation}
        To understand this problem we must analyze the support of the
        joint PDF of $X$ and $Y$. The joint PDF can be obtained by
        \begin{equation}
          \label{eq:3a-joint-pdf}
          f_{X,Y}(x,y) = f_{X}(x)f_{Y|X}(y|x) = \frac{1}{10}(1) \\
        \end{equation}
        Where $f_{Y|X}(y|x)=x+N$ so that
        \begin{equation}
          \label{eq:3a-y-given-x-def}
          Y|X\sim\text{Uniform}(x,x+1)
        \end{equation}
        This generates a parallelogram as the support of the joint
        PDF. We can now attempt to obtain $f_{X|Y}(x|y)$.
        \begin{equation}
          \label{eq:3a-x-given-y-attempt}
          \begin{aligned}
            f_{X|Y}(x|y) &= \frac{f_X(x)f_{Y|X}(y|x)}
            {\int f_X(x)f_{Y|X}(y|x)\dd{x}} \\
            &= \frac{\frac{1}{10} (1)}{\int \frac{1}{10}(1)\dd{x}} \\
            \implies f_{X|Y}(x|y) &= \frac{1}{\int \dd{x}} \\
          \end{aligned}
        \end{equation}
        The conditional PDF becomes piecewise based on the segment of
        the parallelogram that we're currently on
        \begin{equation}
          \label{eq:3a-x-given-y-piecewise}
          f_{X|Y}(x|y) = \left\{
            \begin{aligned}
              & \frac{1}{\int_2^y\dd{x}} &&\quad 2 \le x \le 3 \\
              & \frac{1}{\int_{y-1}^{y}\dd{x}} &&\quad 3 < x < 11 \\
              & \frac{1}{\int_{y-1}^{12}\dd{x}} &&\quad 11 < x < 12 \\
            \end{aligned} \right.
        \end{equation}
        \begin{equation}
          \label{eq:3a-x-given-y-piecewise}
          f_{X|Y}(x|y) = \left\{
            \begin{aligned}
              & \frac{1}{y-2} &&\quad 2 \le x \le 3 \\
              & 1 &&\quad 3 < x < 11 \\
              & \frac{1}{13-y} &&\quad 11 < x < 12 \\
            \end{aligned} \right.
        \end{equation}
        So that now the expectation is easy to find
        \begin{equation}
          \label{eq:3a-expectation-piecewise}
          E[X|Y=y] = \left\{
            \begin{aligned}
              & \int_{2}^{y} \frac{1}{y-2}\dd{x} &&\quad 2 \le x \le 3 \\
              & \int_{y-1}^{y}\dd{x} &&\quad 3 < x < 11 \\
              & \int_{y-1}^{12}\frac{1}{13-y}\dd{x} &&\quad 11 < x < 12 \\
            \end{aligned} \right.
        \end{equation}
        Which turns out to be
        \begin{equation}
          \label{eq:3a-expectation-piecewise-sol}
          \implies E[X|Y=y] = \left\{
            \begin{aligned}
              & \frac{y+2}{2} &&\quad 2 \le x \le 3 \\
              & y-\frac{1}{2} &&\quad 3 < x < 11 \\
              & \frac{11+y}{2} &&\quad 11 < x < 12 \\
            \end{aligned} \right.
        \end{equation}
      \item \hfill \\
        \begin{mdframed}[backgroundcolor=silver]
          \begin{description}
          \item[Mean Square Error] \hfill \\
            $$E[(\Theta-E[\Theta|X])^2|X=x]$$
          \end{description}
        \end{mdframed}
        Finding the Mean Square Error for this problem now just
        requires integration
        \begin{equation}
          \label{eq:3b-mse-def}
          E[(X-E[X|Y=y])^2|Y=y] = \int (X-E[X|Y=y])f_{X|Y}(x|y) \dd{x}
        \end{equation}
        Which we can obtain by integrating the expected values
        piecewise
        \begin{equation}
          \label{eq:3b-mse-piecewise}
          E[(X-E[X|Y])^2|Y] = \left\{
            \begin{aligned}
              & \int_{2}^{y}
              (x-\frac{y+2}{2})^2\left(\frac{1}{y-2}\right) \dd{x}
              &&\quad 2 \le x \le 3 \\
              & \int_{y-1}^{y} \left(x-y+\frac{1}{2}\right)^2(1) \dd{x}
              &&\quad 3 < x < 11 \\
              & \int_{y-1}^{12}
              \left(x-\frac{y+11}{2}\right)^2\left(\frac{1}{13-y}\right) \dd{x}
              &&\quad 11 \le x \le 12 \\
            \end{aligned} \right.
        \end{equation}
        Which turns out to be
        \begin{equation}
          \label{eq:3b-mse-piecewise-sol}
          \implies E[(X-E[X|Y])^2|Y] = \left\{
            \begin{aligned}
              & \frac{(y-2)^2}{12}
              &&\quad 2 \le x \le 3 \\
              & \frac{1}{12}
              &&\quad 3 < x < 11 \\
              & \frac{(y-13)^2}{12}
              &&\quad 11 \le x \le 12 \\
            \end{aligned} \right.
        \end{equation}
      \end{enumerate}
    \end{solution}
  \end{Ex}
\item
  \begin{Ex}
    Romeo and Juliet start dating, but Juliet will be late on any date
    by a random amount X, uniformly distributed over the interval $[0,
    \theta]$. The parameter $\theta$ is unknown and is modeled as the
    value of a random variable $\Theta$, uniformly distributed between
    zero and one hour.
    \begin{enumerate}
    \item Assuming that Juliet was late by an amount $x$ on their
      first date, how should Romeo use this information to update the
      distribution of $\Theta$?
    \item Find the MAP estimate of $\Theta$ based on the observation
      $X = x$.
    \item Find the Least Mean Square estimate of $\Theta$ based on the
      observation $X = x$.
    \item Derive the Linear Least Mean Square estimator of $\Theta$
      based on $X$.
    \end{enumerate}
    \begin{solution} \hfill
      \begin{enumerate}
      \item We were given that $X\sim\text{Uniform}(0,\theta)$ and
        $\Theta\sim\text{Uniform}(0,1)$.

        Since $\Theta$ is uniform, we can define the PDF of $\Theta$
        \begin{equation}
          \label{eq:4a-pdf-theta}
          f_\Theta(\theta)=\left\{
            \begin{aligned}
              & 1 &&\quad 0\le\theta\le1 \\
              & 0 &&\quad \text{otherwise} \\
            \end{aligned}\right.
        \end{equation}
        and the conditional PDF of $X$ given $\Theta$
        \begin{equation}
          \label{eq:4a-pdf-x-given-theta}
          f_{X|\Theta}(x|\theta)=\left\{
            \begin{aligned}
              & \frac{1}{\theta} &&\quad 0\le x \le\theta \\
              & 0 &&\quad \text{otherwise} \\
            \end{aligned}\right.
        \end{equation}

        So now we must obtain the posterior PDF. We can use Bayes'
        Rule.

        \begin{mdframed}[backgroundcolor=silver]
          \begin{description}
          \item[Bayes' Rule] \hfill \\
            $$f_{\Theta|X}(\theta|x) =
            \frac{f_{\Theta}(\theta)f_{X|\Theta}(x|\theta)} {\int
              f_{\Theta}(\theta^{\prime})f_{X|\Theta}(x|\theta^{\prime})
              \dd{\theta^{\prime}}}$$
            Where $f_{\Theta}$ is the prior distribution and
            $f_{X|\Theta}$ is the model of the observation vector
            $X$. After observing the value $x$ of $X$, we form the
            posterior distribution of $\Theta$, using Bayes' Rule.
          \end{description}
        \end{mdframed}

        \begin{equation}
          \label{eq:4a-presol}
          \begin{aligned}
            f_{\Theta|X} &=
            \frac{f_{\Theta}(\theta)f_{X|\Theta}(x|\theta)} {\int
              f_{\Theta}(\theta^{\prime})f_{X|\Theta}(x|\theta^{\prime})
              \dd{\theta^{\prime}}} \\
            &= \frac{(1)(\frac{1}{\theta})}{\int_x^1
                (1)(\frac{1}{\theta}) \dd{\theta^{\prime}}} \\
            &= \frac{1}{\theta}[\ln(\theta^{\prime})]_x^1 \\
            &= \frac{1}{\theta}(\ln(1) - \ln(x)) \\
            &= \frac{1}{\theta}(-\ln(x)) \\
            \implies f_{\Theta|X}&= \frac{1}{\theta\ln(x)} \\
          \end{aligned}
        \end{equation}
        For precision,
        \begin{equation}
          \label{eq:4a-sol}
            \implies f_{\Theta|X} = \left\{
              \begin{aligned}
                & \frac{1}{\theta|\ln(x)|}
                &&\quad 0\le x\le\theta\le 1 \\
                & 0 &&\quad\text{otherwise} \\
              \end{aligned} \right.
        \end{equation}
      \item We must use the MAP estimator.
        \begin{mdframed}[backgroundcolor=silver]
          \begin{description}
          \item[MAP Estimator] \hfill \\
            $$\hat{\theta} = \argmax_{\theta}
            f_{\Theta|X}(\theta|x)$$

            We select a value of $\theta$, denoted $\hat{\theta}$,
            which maximizes the posterior distribution
            $f_{\Theta|X}(\theta|x)$.
          \end{description}
        \end{mdframed}
        In our case, maximizing $f_{\Theta|X}(\theta|x)$ requires us
        to select the lowest value possible for $\hat{\theta}$ since a
        lower value in the denominator of a fraction promotes larger
        values when written in decimal. The lowest possible value we
        may choose for $\hat{\theta}$ in order to maximize
        $f_{\Theta|X}(\theta|x)$ is $\hat{\theta}=x$.
      \item We must use the LMS estimator.
        \begin{mdframed}[backgroundcolor=silver]
          \begin{description}
          \item[LMS Estimator] \hfill \\
            $$\hat{\theta} = E[\Theta|X=x]$$
          \end{description}
        \end{mdframed}
        Utilizing the LMS estimator simply requires some calculation.
        \begin{equation}
          \label{eq:4c-sol}
          \begin{aligned}
            \hat{\Theta} &= E[\Theta|X=x] \\
            &= \int_x^1 \theta \frac{1}{\theta|\ln x|} \dd{\theta} \\
            &= \int_x^1 \frac{1}{|\ln x|} \dd{\theta} \\
            &= \frac{1}{|\ln x|} [\theta]_x^1 \\
            &= \frac{1}{|\ln x|} (1-x) \\
            \implies \hat{\Theta} &= \frac{1-x}{|\ln x|} \\
          \end{aligned}
        \end{equation}
      \item  We must use the LLMS estimator.
        \begin{mdframed}[backgroundcolor=silver]
          \begin{description}
          \item[LLMS Estimator] \hfill \\
            $$\hat{\Theta} = E[\Theta]+
            \frac{\text{Cov}(\Theta,X)}{\text{Var}(X)}
            (X-E[X])$$
            or alternatively
            $$\hat{\Theta} = E[\Theta]+
            \rho\frac{\sigma_{\Theta}}{\sigma_{X}}(X-E[X])$$
            where
            $$\rho =
            \frac{\text{Cov}(\Theta,X)}{\sigma_{\Theta}\sigma_{X}}$$
          \end{description}
        \end{mdframed}
        \begin{equation}
          \label{eq:3c-llms-estimator-sol}
          \begin{aligned}
            \hat{\Theta} &= E[\Theta]+
            \frac{\text{Cov}(\Theta,X)}{\text{Var}(X)} (X-E[X]) \\
            &= \frac{1}{2} + \frac{E[\Theta X] -
              E[\Theta]E[X]}{\frac{\theta^2}{12}}
            \left(X-\frac{\theta}{2}\right) \\
            &= \frac{1}{2} + \frac{E[\Theta X] -
              \frac{\theta}{4}}{\frac{\theta^2}{12}}
            \left(X-\frac{\theta}{2}\right) \\
            &= \frac{1}{2} + \frac{E[E[\Theta X|\Theta]] -
              \frac{\theta}{4}}{\frac{\theta^2}{12}}
            \left(X-\frac{\theta}{2}\right) \\
            &= \frac{1}{2} + \frac{E\left[\Theta
                \frac{1}{\theta}\right] -
              \frac{\theta}{4}}{\frac{\theta^2}{12}}
            \left(X-\frac{\theta}{2}\right) \\
            \implies \hat{\Theta} &= \frac{1}{2} + \frac{1 -
              \frac{\theta}{4}}{\frac{\theta^2}{12}}
            \left(X-\frac{\theta}{2}\right) \\
            \implies \hat{\Theta} &= \frac{2 \theta^2-3 \theta
              (X+2)+12 X}{\theta^2} \\
          \end{aligned}
        \end{equation}
      \end{enumerate}
    \end{solution}
  \end{Ex}
\item
  \begin{Ex}
    Of 500 voters asked whether they are going to vote republican, 260
    said yes.
    \begin{enumerate}
    \item Find a 95\% confidence interval of the probability $p$ that
      the vote for a certain office will be for the Republican
      candidate.
    \item Suppose that 475 of 500 voters said they will vote
      Democrat. Repeat part (a).
    \end{enumerate}
    \begin{solution} \hfill
      \begin{enumerate}
      \item In our case, $\alpha = 0.05$. The confidence interval is
        between $[\hat{\Theta}_n^{-}, \hat{\Theta}_n^{+}]$ for
        $1-\alpha$ so that $p_{\Theta}(\hat{\Theta}_n^{-} \le \theta
        \le \hat{\Theta}_n^{+}) \ge 1-\alpha$.

        We can treat each sample as a Bernoulli RV
        \begin{table}[H]
          \begin{equation}
            \label{eq:5a-rv}
            X_i\sim\text{Bernoulli}(0.52)
          \end{equation}
          \begin{tabularx}{\linewidth}{XX}
            \begin{equation}
              \label{eq:5a-mean}
              \begin{aligned}
                E[X_i] &= 0.52 \\
                \implies \mu_{X_i} &= 0.52 \\
              \end{aligned}
            \end{equation}
            &
            \begin{equation}
              \label{eq:5a-var}
              \begin{aligned}
                \text{Var}(X_i) &= (0.52)(1-0.52) \\
                \implies \sigma_{X_i}^2 &= 0.2496 \\
              \end{aligned}
            \end{equation}
          \end{tabularx}
        \end{table}
        So that when we would like to reference the entire
        distribution $X=\sum X_i = n X_1$
        \begin{table}[H]
          \begin{tabularx}{\linewidth}{XX}
            \begin{equation}
              \label{eq:5a-mean-x}
              \begin{aligned}
                E[X] &= E[n X_i] \\
                \implies \mu_{X} &= 260 \\
              \end{aligned}
            \end{equation}
            &
            \begin{equation}
              \label{eq:5a-var-x}
              \begin{aligned}
                \text{Var}(X) &= \text{Var}(n X_1) \\
                \implies \sigma_{X_i}^2 &= 62400 \\
              \end{aligned}
            \end{equation}
          \end{tabularx}
        \end{table}

        The confidence interval then becomes
        \begin{equation}
          \label{eq:5a-presol}
            \left[260 - 1.96\sqrt{\frac{\sigma_X^2}{n}}, 260 +
            1.96\sqrt{\frac{\sigma_X^2}{n}}\right]
        \end{equation}
        Which makes our interval
        \begin{equation}
          \label{eq:5a-sol}
          \implies [238.104, 281.896]
        \end{equation}
      \item Repeating the above with the altered probabilities yeilds
        \begin{equation}
          \label{eq:5b-presol}
            \left[25 - 1.96\sqrt{\frac{\sigma_X^2}{n}}, 25 +
            1.96\sqrt{\frac{\sigma_X^2}{n}}\right]
        \end{equation}
        Which makes our interval
        \begin{equation}
          \label{eq:5b-sol}
          \implies [15.4481, 35.5519]
        \end{equation}
      \end{enumerate}
    \end{solution}
  \end{Ex}
\item
  \begin{Ex}
    Professor Hoogle always wonders what would be a fair grade for
    students that take his course. Even if you are an outstanding
    student, you usually don’t do well on every assignment. With many
    years of teaching experience, he concludes that the
    ``fluctuations'' of a student’s performance over the assignments,
    i.e., the standard deviation of points a student gets on
    assignments, is fixed to 5 points. This semester he decides to
    give grades according to the mean points of assignments. However
    this would be a sample mean and he wants to have a confidence
    interval less than 6 points with probability 0.95 for the true
    performance of a student, i.e., the true mean of assignment
    points. What is the minimum number of assignments he should give?
    \begin{solution} \hfill
      {\huge \color{red} TODO}
    \end{solution}
  \end{Ex}
\end{enumerate}
\end{document}
