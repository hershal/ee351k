\documentclass[12pt]{article}
\title{EE351K Homework 10}
\author{Hershal Bhave (hb6279)}
\date{Due November 20, 2014}

\usepackage{multicol}
\usepackage{float}
\usepackage[in]{fullpage}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{tabularx}
\usepackage{rotating}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{cleveref}
\usepackage{graphics}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[nosolutionfiles]{answers}
\usepackage[acronym]{glossaries}

\newenvironment{Ex}{\textbf{Problem}\vspace{.75em}\\}{}
\Newassociation{solution}{Soln}{Answers}
\pagebreak[3]
\newcommand{\Opentesthook}[2]{\Writetofile{#1}{\protect\section{#1: #2}}}
\renewcommand{\Solnlabel}[1]{\textbf{Solution}\quad}

\newcommand{\dd}[1]{\:\mathrm{d}{#1}}
\newcommand{\ddt}[1]{\frac{\dd{}}{\dd{#1}}}
\newcommand{\dddt}[1]{\frac{\dd{}^2}{\dd{#1}^2}}

\DeclareMathOperator*{\argmax}{arg\,max}

\definecolor{silver}{rgb}{0.95,0.95,0.95}

\begin{document}
\maketitle
\begin{enumerate}
\item
  \begin{Ex}
    At a computer disk drive factory, inspectors randomly pick a
    product from production lines to detect a failure. If the
    production lines are normal, this failure rate $q_0 =
    10^{-4}$. However occasionally some problems occur in the lines,
    in which case the rate goes up to $q_1 = 10^{-1}$. Let $H_i$
    denote the hypothesis that the failure rate is $q_i$.

    Every morning, an inspector chooses drives at random from the
    previous dayâ€™s production and tests them. If a failure occurs too
    soon, the company stops production and checks the critical part of
    the process. Production line problems occur at random once every
    10 days, so that $P(H_1) = 0.1 = 1 - P(H_0)$.

    \begin{enumerate}
    \item Based on $N$, the number of drives tested up to and
      including the first failure, design a MAP test.
    \item Calculate the probability of ``false alarm'' and the
      probability of ``missed detection''.
    \item Based on this, calculate the probability of detection error
      $P_e$.
    \end{enumerate}
    \begin{solution} \hfill
      \begin{enumerate}
      \item First we will set up the problem. $P(H_0)=p=0.9$ and
        $P(H_1)=1-p=0.1$ so that
        \begin{equation}
          \label{eq:1a-n-dist}
          N\sim\left\{
            \begin{aligned}
              & \text{Geometric}(q_0) && \quad\text{w.p. } p \\
              & \text{Geometric}(q_1) && \quad\text{w.p. } 1-p \\
            \end{aligned} \right.
        \end{equation}
        We will define 0 and 1 to map to $H_0$ and $H_1$
        respectively so that
        \begin{equation}
          \label{eq:1a-h-mapping}
          \begin{aligned}
            P_\Theta(0) = 0.9 \\
            P_\Theta(1) = 0.1 \\
          \end{aligned}
        \end{equation}
        For this problem, we would like to find $\hat{\theta}$ which
        maximizes $P_{\Theta|N}(\theta|n)$.
        \begin{equation}
          \label{eq:1a-theta-hat}
          \hat{\theta}_{\text{MAP}}(n) = \argmax_{\theta \in \{0,1\}}
          P_{\Theta|N}(\theta|n)
        \end{equation}
        Which becomes
        \begin{equation}
          \label{eq:1a-theta-hat-expanded}
          \hat{\theta}_{\text{MAP}} = \left\{
            \begin{aligned}
              & 0 &&\quad n>n^* \\
              & 1 &&\quad n\le n^* \\
            \end{aligned} \right.
        \end{equation}
        So that $H_0$ is chosen if $n>n^*$ and $H_1$ is chosen if
        $n\le n^*$.

        \begin{mdframed}[backgroundcolor=silver]
          \begin{description}
          \item[MAP Rule] \hfill \\
            $$\hat{\theta}_{\text{MAP}}(n) = \argmax_{\theta}
            P_{\Theta|N}(\theta|n)$$
            Equivalently, it selects $\hat{\theta}$ that maximizes
            over $\theta$
            $$f_{\Theta}(\theta)f_{X|\Theta}(x|\theta)$$
            So that if $\theta\in\{0,1\}$, then the corresponding MAP
            test to decide in favor of $\theta=1$ looks like
            $$f_{\Theta}(0)f_{X|\Theta}(x|0) >
            f_{\Theta}(1)f_{X|\Theta}(x|1)$$
          \end{description}
        \end{mdframed}

        The corresponding MAP test is in the form
        \begin{equation}
          \label{eq:1a-map-test}
          \begin{aligned}
            p_{\Theta}(q_0)p_{N|\Theta}(n|q_0)
            &> p_{\Theta}(q_1)p_{N|\Theta}(n|q_1) \\
            P(H_0)p_{N|\Theta}(n|q_0)
            &> P(H_1)p_{N|\Theta}(n|q_1) \\
            \frac{P(H_0)}{P(H_1)}
            &> \frac{p_{N|\Theta}(n|q_1)}{p_{N|\Theta}(n|q_0)} \\
            \frac{P(H_0)}{P(H_1)}
            &> \frac{(1-q_1)^{n-1}(q_1)}{(1-q_0)^{n-1}(q_0)} \\
            \frac{P(H_0)(q_0)}{P(H_1)(q_1)}
            &> \frac{(1-q_1)^{n-1}}{(1-q_0)^{n-1}} \\
            \log\left(\frac{P(H_0)(q_0)}{P(H_1)(q_1)}\right)
            &> \log[(1-q_1)^{n-1}] - \log[(1-q_0)^{n-1}] \\
            \log\left(\frac{P(H_0)(q_0)}{P(H_1)(q_1)}\right)
            &> (n-1)\left(\frac{\log(1-q_1)}{\log(1-q_0)}\right) \\
            n-1 &<
            \frac{\log\left(\frac{P(H_0)(q_0)}{P(H_1)(q_1)}\right)}
            {\left(\frac{\log(1-q_1)}{\log(1-q_0)}\right)} \\
            \implies n^* &= 1 +
            \frac{\log\left(\frac{P(H_0)(q_0)}{P(H_1)(q_1)}\right)}
            {\left(\frac{\log(1-q_1)}{\log(1-q_0)}\right)} \\
            \implies n^* &= 45.7512 \\
          \end{aligned}
        \end{equation}

        This means that after 46 drives are tested, we can make a
        decision about which state of production quality the factory
        is in. If a failure occurs before $n=46$, then we can assume
        that $H_1$ is true and the failure rate of the factory is is
        $q_1$. If the worker tests 46 drives and no failures occur,
        then we can assume that the $H_0$ is true and the failure rate
        of the factory is $q_0$.
      \item The probability of ``false alarm'' is the probability that
        the number of drives until a failure is less than 46 given
        that the factory's production is normal (i.e. $H_0$ is
        true). The probability of ``false alarm'' can be found by the
        following
        \begin{equation}
          \label{eq:1b-false-alarm}
          \begin{aligned}
            P(N\le n^*|H_0) &= \int_1^{n^*} (1-q_0)^{n^*-1}(q_0)\dd{n^*} \\
            P(N\le 46|H_0) &=\int_1^{46} (1-q_0)^{n^*}(q_0)\dd{n^*} \\
            \implies P(N\le 46|H_0) &= 0.00449011 \\
          \end{aligned}
        \end{equation}
        The probability of a ``false alarm'' is less than 0.5\%.

        The probability of a ``missed detection'' is the probability
        that the factory the number of drives untail a failure is
        greater than 46 given that the factory's production is
        abnormal. The probability of a ``missed detection'' can be
        found by the following

        \begin{equation}
          \label{eq:1b-missed-detection}
          \begin{aligned}
            P(N\ge n^*|H_1) &= \int_{n^*}^{\infty}
            (1-q_1)^{n^*-1}(q_1)\dd{n^*} \\
            P(N\ge 46|H_1) &= \int_{46}^{\infty}
            (1-q_1)^{n^*-1}(q_1)\dd{n^*} \\
            \implies P(N\ge 46|H_1) &= 0.00872796 \\
          \end{aligned}
        \end{equation}
        The probability of a ``missed detection'' is less than 0.9\%.
      \item The probabilty of a ``detection error'' is the probability
        that the factory is in the $H_0$ state and there is a false
        alarm or the factory is in the $H_1$ state and there is a
        missed detection.

        % (format "%f" (+ (* 0.9 0.00449011) (* 0.1 0.00872796)))
        \begin{equation}
          \label{eq:1c-detection-error}
          \begin{aligned}
            P_e &= P(H_0)P(N\le n^*|H_0) +  P(H_1)P(N\le n^*|H_1) \\
            &= (0.9)(0.00458967) + (0.1)(0.00872796) \\
            \implies P_e &= 0.0049139 \\
          \end{aligned}
        \end{equation}
      \end{enumerate}
    \end{solution}
  \end{Ex}
\pagebreak[4]
\item
  \begin{Ex}
    Suppose that $X_1,\ldots,X_n$ form a random sample from a uniform
    distribution on the interval $(0,\theta)$, where of the parameter
    $\theta > 0$ but is unknown. Please find MLE of $\theta$.
    \begin{solution} \hfill
      \\ {\huge \color{red} MLE==Maximum Likelihood Estimation?}
    \end{solution}
  \end{Ex}
\item
  \begin{Ex}
    Let $X$ be a random variable uniform over the interval $[2,
    12]$. Suppose we observe $X$ with some random error $N$ which is
    uniformly distributed over interval $[0,1]$. i.e., $Y=X+N$
    \begin{enumerate}
    \item Find the Least mean square estimate of X based on the
      observation $Y = y$.
    \item Find the mean square error.
    \end{enumerate}
    \begin{solution} \hfill
      \begin{enumerate}
      \item Before we attempt this problem we must analyze the data we
        are given. We know:
        \begin{equation}
          \label{eq:3a-defs}
          \begin{aligned}
            X&\sim\text{Uniform}(2,12) \\
            N&\sim\text{Uniform}(0,1) \\
            Y&=X+N \\
          \end{aligned}
        \end{equation}
        To understand this problem we must analyze the support of the
        joint PDF of $X$ and $Y$. The joint PDF can be obtained by
        \begin{equation}
          \label{eq:3a-joint-pdf}
          f_{X,Y}(x,y) = f_{X}(x)f_{Y|X}(y|x) = \frac{1}{10}(1) \\
        \end{equation}
        Where $f_{Y|X}(y|x)=x+N$ so that
        \begin{equation}
          \label{eq:3a-y-given-x-def}
          Y|X\sim\text{Uniform}(x,x+1)
        \end{equation}
        This generates a parallelogram as the support of the joint
        PDF. We can now attempt to obtain $f_{X|Y}(x|y)$.
        \begin{equation}
          \label{eq:3a-x-given-y-attempt}
          \begin{aligned}
            f_{X|Y}(x|y) &= \frac{f_X(x)f_{Y|X}(y|x)}
            {\int f_X(x)f_{Y|X}(y|x)\dd{x}} \\
            &= \frac{\frac{1}{10} (1)}{\int \frac{1}{10}(1)\dd{x}} \\
            \implies f_{X|Y}(x|y) &= \frac{1}{\int \dd{x}} \\
          \end{aligned}
        \end{equation}
        The conditional PDF becomes piecewise based on the segment of
        the parallelogram that we're currently on
        \begin{equation}
          \label{eq:3a-x-given-y-piecewise}
          f_{X|Y}(x|y) = \left\{
            \begin{aligned}
              & \frac{1}{\int_2^y\dd{x}} &&\quad 2 \le x \le 3 \\
              & \frac{1}{\int_{y-1}^{y}\dd{x}} &&\quad 3 < x < 11 \\
              & \frac{1}{\int_{y-1}^{12}\dd{x}} &&\quad 11 < x < 12 \\
            \end{aligned} \right.
        \end{equation}
        \begin{equation}
          \label{eq:3a-x-given-y-piecewise}
          f_{X|Y}(x|y) = \left\{
            \begin{aligned}
              & \frac{1}{y-2} &&\quad 2 \le x \le 3 \\
              & 1 &&\quad 3 < x < 11 \\
              & \frac{1}{13-y} &&\quad 11 < x < 12 \\
            \end{aligned} \right.
        \end{equation}
        So that now the expectation is easy to find
        \begin{equation}
          \label{eq:3a-expectation-piecewise}
          E[X|Y=y] = \left\{
            \begin{aligned}
              & \int_{2}^{y} \frac{1}{y-2}\dd{x} &&\quad 2 \le x \le 3 \\
              & \int_{y-1}^{y}\dd{x} &&\quad 3 < x < 11 \\
              & \int_{y-1}^{12}\frac{1}{13-y}\dd{x} &&\quad 11 < x < 12 \\
            \end{aligned} \right.
        \end{equation}
        Which turns out to be
        \begin{equation}
          \label{eq:3a-expectation-piecewise-sol}
          \implies E[X|Y=y] = \left\{
            \begin{aligned}
              & \frac{y+2}{2} &&\quad 2 \le x \le 3 \\
              & y-\frac{1}{2} &&\quad 3 < x < 11 \\
              & \frac{11+y}{2} &&\quad 11 < x < 12 \\
            \end{aligned} \right.
        \end{equation}
      \item \hfill \\
        \begin{mdframed}[backgroundcolor=silver]
          \begin{description}
          \item[Mean Square Error] \hfill \\
            $$E[(\Theta-E[\Theta|X])^2|X=x]$$
          \end{description}
        \end{mdframed}
        Finding the Mean Square Error for this problem now just
        requires integration
        \begin{equation}
          \label{eq:3b-mse-def}
          E[(X-E[X|Y=y])^2|Y=y] = \int (X-E[X|Y=y])f_{X|Y}(x|y) \dd{x}
        \end{equation}
        Which we can obtain by integrating the expected values
        piecewise
        \begin{equation}
          \label{eq:3b-mse-piecewise}
          E[(X-E[X|Y])^2|Y] = \left\{
            \begin{aligned}
              & \int_{2}^{y}
              (x-\frac{y+2}{2})^2\left(\frac{1}{y-2}\right) \dd{x}
              &&\quad 2 \le x \le 3 \\
              & \int_{y-1}^{y} \left(x-y+\frac{1}{2}\right)^2(1) \dd{x}
              &&\quad 3 < x < 11 \\
              & \int_{y-1}^{12}
              \left(x-\frac{y+11}{2}\right)^2\left(\frac{1}{13-y}\right) \dd{x}
              &&\quad 11 \le x \le 12 \\
            \end{aligned} \right.
        \end{equation}
        Which turns out to be
        \begin{equation}
          \label{eq:3b-mse-piecewise-sol}
          \implies E[(X-E[X|Y])^2|Y] = \left\{
            \begin{aligned}
              & \frac{(y-2)^2}{12}
              &&\quad 2 \le x \le 3 \\
              & \frac{1}{12}
              &&\quad 3 < x < 11 \\
              & \frac{(y-13)^2}{12}
              &&\quad 11 \le x \le 12 \\
            \end{aligned} \right.
        \end{equation}
      \end{enumerate}
    \end{solution}
  \end{Ex}
\item
  \begin{Ex}
    Romeo and Juliet start dating, but Juliet will be late on any date
    by a random amount X, uniformly distributed over the interval $[0,
    \theta]$. The parameter $\theta$ is unknown and is modeled as the
    value of a random variable $\Theta$, uniformly distributed between
    zero and one hour.
    \begin{enumerate}
    \item Assuming that Juliet was late by an amount $x$ on their
      first date, how should Romeo use this information to update the
      distribution of $\Theta$?
    \item Find the MAP estimate of $\Theta$ based on the observation
      $X = x$.
    \item Find the Least Mean Square estimate of $\Theta$ based on the
      observation $X = x$.
    \item Derive the Linear Least Mean Square estimator of $\Theta$
      based on $X$.
    \end{enumerate}
    \begin{solution} \hfill
      \begin{enumerate}
      \item We were given that $X\sim\text{Uniform}(0,\theta)$ and
        $\Theta\sim\text{Uniform}(0,1)$.

        Since $\Theta$ is uniform, we can define the PDF of $\Theta$
        \begin{equation}
          \label{eq:4a-pdf-theta}
          f_\Theta(\theta)=\left\{
            \begin{aligned}
              & 1 &&\quad 0\le\theta\le1 \\
              & 0 &&\quad \text{otherwise} \\
            \end{aligned}\right.
        \end{equation}
        and the conditional PDF of $X$ given $\Theta$
        \begin{equation}
          \label{eq:4a-pdf-x-given-theta}
          f_{X|\Theta}(x|\theta)=\left\{
            \begin{aligned}
              & \frac{1}{\theta} &&\quad 0\le x \le\theta \\
              & 0 &&\quad \text{otherwise} \\
            \end{aligned}\right.
        \end{equation}

        So now we must obtain the posterior PDF. We can use Bayes'
        Rule.

        \begin{mdframed}[backgroundcolor=silver]
          \begin{description}
          \item[Bayes' Rule] \hfill \\
            $$f_{\Theta|X}(\theta|x) =
            \frac{f_{\Theta}(\theta)f_{X|\Theta}(x|\theta)} {\int
              f_{\Theta}(\theta^{\prime})f_{X|\Theta}(x|\theta^{\prime})
              \dd{\theta^{\prime}}}$$
            Where $f_{\Theta}$ is the prior distribution and
            $f_{X|\Theta}$ is the model of the observation vector
            $X$. After observing the value $x$ of $X$, we form the
            posterior distribution of $\Theta$, using Bayes' Rule.
          \end{description}
        \end{mdframed}

        \begin{equation}
          \label{eq:4a-presol}
          \begin{aligned}
            f_{\Theta|X} &=
            \frac{f_{\Theta}(\theta)f_{X|\Theta}(x|\theta)} {\int
              f_{\Theta}(\theta^{\prime})f_{X|\Theta}(x|\theta^{\prime})
              \dd{\theta^{\prime}}} \\
            &= \frac{(1)(\frac{1}{\theta})}{\int_x^1
                (1)(\frac{1}{\theta}) \dd{\theta^{\prime}}} \\
            &= \frac{1}{\theta}[\ln(\theta^{\prime})]_x^1 \\
            &= \frac{1}{\theta}(\ln(1) - \ln(x)) \\
            &= \frac{1}{\theta}(-\ln(x)) \\
            \implies f_{\Theta|X}&= \frac{1}{\theta\ln(x)} \\
          \end{aligned}
        \end{equation}
        For precision,
        \begin{equation}
          \label{eq:4a-sol}
            \implies f_{\Theta|X} = \left\{
              \begin{aligned}
                & \frac{1}{\theta|\ln(x)|}
                &&\quad 0\le x\le\theta\le 1 \\
                & 0 &&\quad\text{otherwise} \\
              \end{aligned} \right.
        \end{equation}
      \item We must use the MAP estimator.
        \begin{mdframed}[backgroundcolor=silver]
          \begin{description}
          \item[MAP Estimator] \hfill \\
            $$\hat{\theta} = \argmax_{\theta}
            f_{\Theta|X}(\theta|x)$$

            We select a value of $\theta$, denoted $\hat{\theta}$,
            which maximizes the posterior distribution
            $f_{\Theta|X}(\theta|x)$.
          \end{description}
        \end{mdframed}
        In our case, maximizing $f_{\Theta|X}(\theta|x)$ requires us
        to select the lowest value possible for $\hat{\theta}$ since a
        lower value in the denominator of a fraction promotes larger
        values when written in decimal. The lowest possible value we
        may choose for $\hat{\theta}$ in order to maximize
        $f_{\Theta|X}(\theta|x)$ is $\hat{\theta}=x$.
      \item We must use the LMS estimator.
        \begin{mdframed}[backgroundcolor=silver]
          \begin{description}
          \item[LMS Estimator] \hfill \\
            $$\hat{\theta} = E[\Theta|X=x]$$
          \end{description}
        \end{mdframed}
        Utilizing the LMS estimator simply requires some calculation.
        \begin{equation}
          \label{eq:4c-sol}
          \begin{aligned}
            \hat{\Theta} &= E[\Theta|X=x] \\
            &= \int_x^1 \theta \frac{1}{\theta|\ln x|} \dd{\theta} \\
            &= \int_x^1 \frac{1}{|\ln x|} \dd{\theta} \\
            &= \frac{1}{|\ln x|} [\theta]_x^1 \\
            &= \frac{1}{|\ln x|} (1-x) \\
            \implies \hat{\Theta} &= \frac{1-x}{|\ln x|} \\
          \end{aligned}
        \end{equation}
      \item  We must use the LLMS estimator.
        \begin{mdframed}[backgroundcolor=silver]
          \begin{description}
          \item[LLMS Estimator] \hfill \\
            $$\hat{\Theta} = E[\Theta]+
            \frac{\text{Cov}(\Theta,X)}{\text{Var}(X)}
            (X-E[X])$$
            or alternatively
            $$\hat{\Theta} = E[\Theta]+
            \rho\frac{\sigma_{\Theta}}{\sigma_{X}}(X-E[X])$$
            where
            $$\rho =
            \frac{\text{Cov}(\Theta,X)}{\sigma_{\Theta}\sigma_{X}}$$
          \end{description}
        \end{mdframed}
        \begin{equation}
          \label{eq:3c-llms-estimator-sol}
          \begin{aligned}
            \hat{\Theta} &= E[\Theta]+
            \frac{\text{Cov}(\Theta,X)}{\text{Var}(X)} (X-E[X]) \\
            &= \frac{1}{2} + \frac{E[\Theta X] -
              E[\Theta]E[X]}{\frac{\theta^2}{12}}
            \left(X-\frac{\theta}{2}\right) \\
            &= \frac{1}{2} + \frac{E[\Theta X] -
              \frac{\theta}{4}}{\frac{\theta^2}{12}}
            \left(X-\frac{\theta}{2}\right) \\
            &= \frac{1}{2} + \frac{E[E[\Theta X|\Theta]] -
              \frac{\theta}{4}}{\frac{\theta^2}{12}}
            \left(X-\frac{\theta}{2}\right) \\
            &= \frac{1}{2} + \frac{E\left[\Theta
                \frac{1}{\theta}\right] -
              \frac{\theta}{4}}{\frac{\theta^2}{12}}
            \left(X-\frac{\theta}{2}\right) \\
            \implies \hat{\Theta} &= \frac{1}{2} + \frac{1 -
              \frac{\theta}{4}}{\frac{\theta^2}{12}}
            \left(X-\frac{\theta}{2}\right) \\
            \implies \hat{\Theta} &= \frac{2 \theta^2-3 \theta
              (X+2)+12 X}{\theta^2} \\
          \end{aligned}
        \end{equation}
      \end{enumerate}
    \end{solution}
  \end{Ex}
\item
  \begin{Ex}
    Of 500 voters asked whether they are going to vote republican, 260
    said yes.
    \begin{enumerate}
    \item Find a 95\% confidence interval of the probability $p$ that
      the vote for a certain office will be for the Republican
      candidate.
    \item Suppose that 475 of 500 voters said they will vote
      Democrat. Repeat part (a).
    \end{enumerate}
    \begin{solution} \hfill
      \begin{enumerate}
      \item In our case, $\alpha = 0.05$. The confidence interval is
        between $[\hat{\Theta}_n^{-}, \hat{\Theta}_n^{+}]$ for
        $1-\alpha$ so that $p_{\Theta}(\hat{\Theta}_n^{-} \le \theta
        \le \hat{\Theta}_n^{+}) \ge 1-\alpha$.

        We can treat each sample as a Bernoulli RV
        \begin{table}[H]
          \begin{equation}
            \label{eq:5a-rv}
            X_i\sim\text{Bernoulli}(0.52)
          \end{equation}
          \begin{tabularx}{\linewidth}{XX}
            \begin{equation}
              \label{eq:5a-mean}
              \begin{aligned}
                E[X_i] &= 0.52 \\
                \implies \mu_{X_i} &= 0.52 \\
              \end{aligned}
            \end{equation}
            &
            \begin{equation}
              \label{eq:5a-var}
              \begin{aligned}
                \text{Var}(X_i) &= (0.52)(1-0.52) \\
                \implies \sigma_{X_i}^2 &= 0.2496 \\
              \end{aligned}
            \end{equation}
          \end{tabularx}
        \end{table}
        So that when we would like to reference the entire
        distribution $X=\sum X_i = n X_1$
        \begin{table}[H]
          \begin{tabularx}{\linewidth}{XX}
            \begin{equation}
              \label{eq:5a-mean-x}
              \begin{aligned}
                E[X] &= E[n X_i] \\
                \implies \mu_{X} &= 260 \\
              \end{aligned}
            \end{equation}
            &
            \begin{equation}
              \label{eq:5a-var-x}
              \begin{aligned}
                \text{Var}(X) &= \text{Var}(n X_1) \\
                \implies \sigma_{X_i}^2 &= 62400 \\
              \end{aligned}
            \end{equation}
          \end{tabularx}
        \end{table}

        The confidence interval then becomes
        \begin{equation}
          \label{eq:5a-presol}
            \left[260 - 1.96\sqrt{\frac{\sigma_X^2}{n}}, 260 +
            1.96\sqrt{\frac{\sigma_X^2}{n}}\right]
        \end{equation}
        Which makes our interval
        \begin{equation}
          \label{eq:5a-sol}
          \implies [238.104, 281.896]
        \end{equation}
      \item Repeating the above with the altered probabilities yeilds
        \begin{equation}
          \label{eq:5b-presol}
            \left[25 - 1.96\sqrt{\frac{\sigma_X^2}{n}}, 25 +
            1.96\sqrt{\frac{\sigma_X^2}{n}}\right]
        \end{equation}
        Which makes our interval
        \begin{equation}
          \label{eq:5b-sol}
          \implies [15.4481, 35.5519]
        \end{equation}
      \end{enumerate}
    \end{solution}
  \end{Ex}
\item
  \begin{Ex}
    Professor Hoogle always wonders what would be a fair grade for
    students that take his course. Even if you are an outstanding
    student, you usually donâ€™t do well on every assignment. With many
    years of teaching experience, he concludes that the
    ``fluctuations'' of a studentâ€™s performance over the assignments,
    i.e., the standard deviation of points a student gets on
    assignments, is fixed to 5 points. This semester he decides to
    give grades according to the mean points of assignments. However
    this would be a sample mean and he wants to have a confidence
    interval less than 6 points with probability 0.95 for the true
    performance of a student, i.e., the true mean of assignment
    points. What is the minimum number of assignments he should give?
    \begin{solution} \hfill
      {\huge \color{red} TODO}
    \end{solution}
  \end{Ex}
\end{enumerate}
\end{document}
