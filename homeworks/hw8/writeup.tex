\documentclass[12pt]{article}
\title{EE351K Homework 8}
\author{Hershal Bhave (hb6279)}
\date{Due October 30, 2014}

\usepackage{multicol}
\usepackage[in]{fullpage}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{rotating}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{cleveref}
\usepackage{graphics}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[nosolutionfiles]{answers}
\usepackage[acronym]{glossaries}

\newenvironment{Ex}{\textbf{Problem}\vspace{.75em}\\}{}
\Newassociation{solution}{Soln}{Answers}
\pagebreak[3]
\newcommand{\Opentesthook}[2]{\Writetofile{#1}{\protect\section{#1: #2}}}
\renewcommand{\Solnlabel}[1]{\textbf{Solution}\quad}

\newcommand{\dd}[1]{\:\mathrm{d}{#1}}
\newcommand{\ddt}[1]{\frac{\dd{}}{\dd{#1}}}
\newcommand{\dddt}[1]{\frac{\dd{}^2}{\dd{#1}^2}}

\begin{document}
\maketitle
\begin{enumerate}
\item
  \begin{Ex}
    Suppose the joint probability density function of two random
    variables $X$ and $Y$ is given by
    \begin{equation}
      \label{eq:1-question}
      f_{X,Y}(x,y) = \left\{
        \begin{aligned}
          & 2 &&\quad 0\le y \le x \le 1 \\
          & 0 &&\quad \text{otherwise} \\
        \end{aligned} \right.
    \end{equation}
    Find $E[X|Y]$ and $\text{Var}[X|Y]$. Recall that $E[X|Y]$ is a
    random variable, which is a fuction of $Y$.
    \begin{solution}
      First we must find the conditional PDF
      \begin{equation}
        \label{eq:1-conditional-pdf-def}
        f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}
      \end{equation}
      Where the marginal $f_Y(y)$ is
      \begin{equation}
        \label{eq:1-y-pdf}
        \begin{aligned}
          f_Y(y) &= \int_y^1 2\dd{x} \\
          &= [2x]_y^1 \\
          \implies f_Y(y) &= 2(1-y) \\
        \end{aligned}
      \end{equation}
      We can use the marginal obtained from \cref{eq:1-y-pdf} and insert
      it into the conditional pdf described into
      \cref{eq:1-conditional-pdf-def}
      \begin{equation}
        \label{eq:1-pdf-plugged}
        \begin{aligned}
          f_{X|Y}(x|y) &= \frac{f_{X,Y}(x,y)}{f_Y(y)} \\
          &= \frac{2}{2(1-y)} \\
          \implies f_{X|Y}(x|y) &= \frac{1}{1-y} \\
        \end{aligned}
      \end{equation}
      Now computing the conditional expectation:
      \begin{equation}
        \label{eq:1-conditional-expectation}
        \begin{aligned}
          E[X|Y] &= \int_y^1 x f_{X|Y}(x|y) \dd{x} \\
          &= \frac{1}{1-y} \int_y^1 x \dd{x} \\
          &= \frac{1}{1-y} \left[\frac{x^2}{2}\right]_y^1 \\
          &= \frac{1}{2(1-y)} (1-y^2) \\
          &= \frac{1}{2(1-y)} (1-y)(1+y) \\
          &= \frac{1+y}{2} \\
          \implies E[X|Y] &= \frac{1+Y}{2} \\
        \end{aligned}
      \end{equation}
      {\color{red} \huge FIND CONDITIONAL VARIANCE}
    \end{solution}
  \end{Ex}
\item
  \begin{Ex}
    Let $X$, $Y$, and $Z$ be jointly discrete random variables. Show
    that
    \begin{equation}
      \label{eq:2-question}
      E[Z] = E[E[Z|X,Y]]
    \end{equation}
    Recall that
    \begin{equation}
      \label{eq:2-recall}
      E[Z|X=x,Y=y] = \sum_z zP_{Z|X,Y}(z|x,y)
    \end{equation}
    Let
    \begin{equation}
      \label{eq:2-let}
      \gamma(x,y)=E[Z|X=x,Y=y]
    \end{equation}
    Then
    \begin{equation}
      \label{eq:2-then}
      E[Z|X,Y] = \gamma(X,Y)
    \end{equation}
    \begin{solution} \hfill
      \begin{equation}
        \label{eq:2-first-sum}
        \begin{aligned}
          E[E[Z|X,Y]] &= E[\gamma(X,Y)] \\
          &= \sum_{x,y} \gamma(X,Y)p_{X,Y}(x,y) \\
          &= \sum_{x,y} E[Z|X,Y] p_{X,Y}(x,y) \\
          &= \sum_{x,y} p_{X,Y}(x,y) \sum_{z} z p_{Z|X,Y}(z|x,y) \\
          &= \sum_{z} z \sum_{x,y} p_{X,Y}(x,y) p_{Z|X,Y}(z|x,y) \\
          &= \sum_{z} z \sum_{x,y} p_{X,Y,Z}(x,y,z) \\
          &= \sum_{z} z p_Z(z) \\
          \implies E[E[Z|X,Y]] &= E[Z] \\
        \end{aligned}
      \end{equation}
    \end{solution}
  \end{Ex}
\item
  \begin{Ex}
    You roll a fair six-sided die, and then you flip a fair coin the
    number of times shown by the die. Find the mean and the variance
    of the number of heads obtained.
    \begin{solution} \hfill \vspace{.75em} \\
      First, let's define a few variables:
      \begin{equation}
        \label{eq:3-decls}
        \begin{aligned}
          D &= \text{ the dice roll outcome, i.e. the number of rolls} \\
          H &= \text{ number of coin flips resulting in heads} \\
        \end{aligned}
      \end{equation}
      The number of heads obtained is the conditional expected value
      $E[H|D]$. When $D=d$, then the distribution of $H$ is binomial
      with $p=0.5$ such that $E[H|D]=0.5D$ and $\text{Var}(H|D) =
      0.25D$. We want to obtain the mean and variance of these
      distributions, respectively
      \\
      \begin{tabularx}{.95\textwidth}{XX}
        \begin{equation}
          \label{eq:3-e}
          \begin{aligned}
            E[E[H|D]] &= E[H] \\
            &= 0.5E[D] \\
            &= 0.5(3.5) \\
            \implies E[E[H|D]] &= 1.75 \\
          \end{aligned}
        \end{equation} &
        \begin{equation}
          \label{eq:3-var}
          \begin{aligned}
            \text{Var}(H|D) &= \text{Var}(H) \\
            &= 0.25(E[D^2] - E[D]^2) \\
            &= 0.25(91/6 - 3.5^2) \\
            \text{Var}(H) &= 0.7292 \\
          \end{aligned}
        \end{equation}
      \end{tabularx}
    \end{solution}
  \end{Ex}
  \pagebreak[3]
\item
  \begin{Ex}
    A certain Sport club has $N$ members, where $N$ is a random
    variable with PMF
    \begin{equation}
      \label{eq:4-question-0}
      p_N(n) = p^{n-1}(1-p) \quad n=1,2,\ldots
    \end{equation}
    On the second Tuesday night of every month, the club holds a
    meeting. Each member attends the meeting with probability $q$,
    independently of all the other members. If a member $i$ attends
    the meeting, then he/she brings an amount of money, $M_i$, which is a
    continuous random variable with PDF
    \begin{equation}
      \label{eq:4-question-1}
      f_{M_{i}}(m) = \lambda e^{-\lambda m}\quad m\ge0
    \end{equation}
    % $N$, $M_i$, and whether each member attends are all
    % independent.
    Determine:
    \begin{enumerate}
    \item The expectation and variance of the number of members
      showing up to the meeting.
    \item The expectation and variance for the total amount of money
      brought to the meeting.
    \end{enumerate}
    \begin{solution} \hfill
      \begin{enumerate}
      \item The expected value of the number of members showing up to
        the meeting can be modeled by
        \begin{equation}
          \label{eq:4a-expectation-p}
          E[qN] = q E[N]
        \end{equation}
        Where $E[N]$ can be modeled by a geometric distribution where
        $z=(1-p)$ such that
        \begin{equation}
          \label{eq:4a-pmf-z}
          p_N(n) = (1-z)^{n-1}(z)
        \end{equation}
        The expected value of a geometric distribution is known as
        \begin{equation}
          \label{eq:4a-expectation-z}
          \begin{aligned}
            E[qN] &= \frac{q}{z} \\
            \implies E[qN] &= \frac{q}{1-p} \\
          \end{aligned}
        \end{equation}
        The variance can be modeled by
        \begin{equation}
          \label{eq:4-variance}
          \begin{aligned}
            \text{Var}(qN) &= q^2\text{Var}(N) \\
            &= q^2\frac{1-z}{z^2} \\
            &= q^2\frac{1-(1-p)}{(1-p)^2} \\
            \implies \text{Var}(qN) &= \frac{q^2p}{(1-p)^2} \\
          \end{aligned}
        \end{equation}
      \item Given that $Y=M_1 + \cdots + M_N$, and $N$ is the number
        of members show show up to the meeting, the expectation of the
        total money broght can be modeled by
        \begin{equation}
          \label{eq:4b-expectation}
          \begin{aligned}
            E[Y|N=n] &= E[M_1 + \cdots + M_N | N=n] \\
            &= E[M_1 + \cdots + M_n | N=n] \\
            &= E[M_1 + \cdots + M_n] \\
            &= n E[M] \\
            &= \left(\frac{q}{1-p}\right)E[M] \\
            \implies E[Y|N=n] &= \frac{q}{(1-p)\lambda} \\
          \end{aligned}
        \end{equation}
        The variance can be computed similarly
        \begin{equation}
          \label{eq:4b-variance}
          \begin{aligned}
            \text{Var}(Y|N=n) &= \text{Var}(M_1 + \cdots + M_N|N=n) \\
            &= \text{Var}(M_1 + \cdots + M_n) \\
            &= n\text{Var}(M) \\
            &= \left(\frac{q}{1-p}\right)
            \left(\frac{1}{\lambda^2}\right) \\
            \implies \text{Var}(Y|N=n) &= \frac{q}{(1-p)\lambda^2} \\
          \end{aligned}
        \end{equation}
      \end{enumerate}
    \end{solution}
  \end{Ex}
\item
  \begin{Ex}
    A random variable $X \sim N(0,1)$ and $Y \sim N(1,2)$ and $X$ and
    $Y$ are independent. Suppose random variables $U$ and $V$ are
    given by $U=X+Y$, $V=X+\alpha Y$ for some $\alpha$.
    \begin{enumerate}
    \item What is the joint distribution of $U,V$ in terms of
      $\alpha$?
    \item Find $\alpha$ (if it exists) such that $U$ and $V$ be
      independent.
    \item Find the conditional distribution $f_{U|V}(u|V = 5)$.
    \end{enumerate}
    \begin{solution} \hfill
      \begin{enumerate}
      \item The joint distribution of $U$, $V$ in terms of $\alpha$ is
        a Bivariate Normal Distribution which looks like
        \begin{equation}
          \label{eq:5a-biv-decl}
          (U,V) \sim N(\mu_U, \mu_V, \sigma_U^2, \sigma_V^2, \rho_{U,V})
        \end{equation}
        where
        \begin{equation}
          \label{eq:5a-rho-decl}
          \rho_{U,V} = \frac{\text{Cov}(U,V)}{\sigma_U \sigma_V}
        \end{equation}
        and
        \begin{equation}
          \label{eq:5a-cov-decl}
          \text{Cov}(U,V) = E[UV]-\mu_U \mu_V
        \end{equation}
        We must first obtain the expected values for both $U$ and
        $V$. \\
        \begin{tabularx}{.95\textwidth}{XX}
          \begin{equation}
            \label{eq:5a-e-u}
            \begin{aligned}
              E[U] &= E[X+Y] \\
              &= E[X] + E[Y] \\
              \implies E[U] &= 1 \\
            \end{aligned}
          \end{equation} &
          \begin{equation}
            \label{eq:5a-e-v}
            \begin{aligned}
              E[V] &= E[X + \alpha Y] \\
              &= E[X] + \alpha E[Y] \\
              \implies E[V] &= \alpha \\
            \end{aligned}
          \end{equation}
        \end{tabularx}
        And the variances for both $U$ and $V$.
        \begin{equation}
          \label{eq:5a-var-u}
          \begin{aligned}
            \text{Var}(U) &= \text{Var}(X+Y) \\
            &= \text{Var}(X) + \text{Var}(Y) + 2 \text{Cov}(X,Y) \\
            \implies \text{Var}(U) &= 3 \\
          \end{aligned}
        \end{equation}
        \begin{equation}
          \label{eq:5a-var-v}
          \begin{aligned}
            \text{Var}(V) &= \text{Var}(X + \alpha Y) \\
            &= \text{Var}(X) + \alpha^2 \text{Var}(Y) + 2
            \text{Cov}(X,\alpha Y) \\
            \implies \text{Var}(V) &= 1+2\alpha^2 \\
          \end{aligned}
        \end{equation}
        We must use the equation in \cref{eq:5a-cov-decl} in order to
        obtain the coveriance of $U$ and $V$ for \cref{eq:5a-rho-decl}.
        \begin{equation}
          \label{eq:5a-cov-aligned}
          \begin{aligned}
            \text{Cov}(U,V) &= E[UV] - \mu_U \mu_V \\
            &= E[(X+Y)(X+\alpha Y)] - \alpha \\
            &= E[X^2 + (1+\alpha)XY + \alpha Y^2] - \alpha \\
            &= E[X^2] + (1+\alpha)E[XY] + \alpha E[Y^2] - \alpha \\
            &= \text{Var}(X) + \alpha(E[Y]^2 + \text{Var}(Y)) -
            \alpha \\
            \implies \text{Cov}(U,V) &= 1 + 2\alpha \\
          \end{aligned}
        \end{equation}
        Plugging this into \cref{eq:5a-rho-decl}
        \begin{equation}
          \label{eq:5a-rho}
          \begin{aligned}
            \rho_{U,V} &= \frac{\text{Cov}(U,V)}{\sigma_U \sigma_V}
            \\
            &= \frac{1 + 2\alpha}{\sqrt{3+6\alpha^2}} \\
          \end{aligned}
        \end{equation}
        So finally the joint distribution of $U$, $V$ in terms of
        $\alpha$ is
        \begin{equation}
          \label{eq:5a-sol}
          \implies f_{U,V} = N(1, \alpha, 3, 1+2\alpha^2, \frac{1 +
            2\alpha}{\sqrt{3+6\alpha^2}})
        \end{equation}
      \item For a jointly Gaussian distribution (Bivariate normal),
        $\rho_{X,Y} = 0$ implies $X$ and $Y$ are independent in
        general. Thus for our problem, making sure that $U$ and $V$
        are independent means we must make $\rho_{U,V} = 0$. Setting
        \cref{eq:5a-rho-decl} equal to zero implies that
        \cref{eq:5a-cov-decl} is equal to zero.
        \begin{equation}
          \label{eq:5b-cov}
          \begin{aligned}
            0 &= \text{Cov}(U,V) \\
            &= 1 + 2 \alpha \\
            \implies \alpha &= -\frac{1}{2} \\
          \end{aligned}
        \end{equation}
      \item We are tasked with finding $f_{U|V}(u|V=5)$, which is
        described as the following.
        \begin{equation}
          \label{eq:5c-presol}
          f_{U|V}(u|V=5) \sim N(\mu_{U|V}(v), \sigma_{U|V}(v))
        \end{equation}
        First we must find $\mu_{U|V}$
        \begin{equation}
          \label{eq:5c-mu-v}
          \begin{aligned}
            \mu_{U|V}(v) &= \mu_V + \rho_{U,V}
            \frac{\sigma_U}{\sigma_V}(v-mu_v) \\
            &= 1 + \frac{1+2\alpha}{\sqrt{3+6\alpha^2}}
            \frac{\sqrt{3}}{\sqrt{1+2\alpha^2}}(5-\alpha)\\
            \implies \mu_{U|V}(v) &= 1 + \frac{1 + 2\alpha}{1 +
              2\alpha^2}(5-\alpha) \\
          \end{aligned}
        \end{equation}
      \end{enumerate}
      And the variance $\sigma_{U|V}^2$
      \begin{equation}
        \label{eq:5c-sigma}
        \begin{aligned}
          \sigma_{U|V}^2 &= \sigma_U^2(1-\rho_{U,V}^2) \\
          &= 3\left(1 -
            \left(\frac{1+2\alpha}{\sqrt{3+6\alpha^2}}\right)\right) \\
          &= 3\left(1- \frac{(1+2\alpha)^2}{3+6\alpha}\right) \\
          \implies \sigma_{U|V}^2 &= 3 - \frac{(1+2\alpha^2)}{1 +
            2\alpha^2} \\
        \end{aligned}
      \end{equation}
      The final solution can be obtained by putting
      \cref{eq:5c-mu-v,eq:5c-sigma} into \cref{eq:5c-presol}.
      \begin{equation}
        \label{eq:5c-sol}
        f_{U|V}(u|V=5) \sim N\left(1 + \frac{1 + 2\alpha}{1 +
            2\alpha^2}(5-\alpha), \;3 - \frac{(1+2\alpha^2)}{1 +
            2\alpha^2}\right)
      \end{equation}
    \end{solution}
  \end{Ex}
\pagebreak[4]
\item
  \begin{Ex}
    Suppose $X$ is a Poisson variable with mean 10. Use Chebyshev's
    Inequality to give a lower bound of $P(5 < X < 15)$.
    \begin{solution} \hfill \vspace{.75em} \\
      Chebyshev's Inequality states that
      \begin{equation}
        \label{eq:6-chebyshev-decl}
        P(|X-\mu_X| > d) \le \frac{\sigma_X^2}{d^2}
      \end{equation}
    \end{solution}
    Since the lower and upper bounds of the requested probability is
    centered equally about the mean, this is a perfect candidate for
    Chebyshev's inequality.
    \begin{equation}
      \label{eq:6-chebyshev}
      \begin{aligned}
        P(5<X<15) &= P(|X-10| < 5) \\
        &= 1 - P(|X-10|\ge 5) \\
        &\le 1 - \frac{\sigma_X^2}{d^2} \\
        &\le 1 - \frac{10}{25} \\
        \implies P(5<X<15) &\le 0.6 \\
      \end{aligned}
    \end{equation}
  \end{Ex}
\item
  \begin{Ex}
    There are 10,000 light bulbs on an LED panel. Suppose that the
    lifetime of each light bulb is exponentially distributed with mean
    50 days, and assume that the light bulbs work independently of one
    another. Suppose that we turn all the light bulbs on and keep them
    on until they burn out. Use Markov's Inequality to give an upper
    bound of the probability that at least 8000 light bulbs are still
    on at the end of the 60th day after we turn them on.
    \begin{solution} \hfill
      In our case we can obtain the PDF of the distribution by
      obtaining $\lambda$. Since we already have the mean we can
      determine $\lambda$ by using the $E[X]$ equation for an
      exponential RV
      \begin{equation}
        \label{eq:7-lambda}
        \begin{aligned}
          E[X] &= 50 \\
          \frac{1}{\lambda} &= 50 \\
          \implies \lambda &= \frac{1}{50} \\
        \end{aligned}
      \end{equation}
      So the probability of any individual bulb lasting more than 60
      days can be calculated by the following
      \begin{equation}
        \label{eq:7-p-60}
        \begin{aligned}
          P(X \ge 60) &= 1 - P(X \le 60) \\
          &= 1 - F_X(60) \\
          &= 1 - (1 - e^{-60\lambda}) \\
          \implies P(X \ge 60) &= e^{-\frac{60}{50}} \\
        \end{aligned}
      \end{equation}
      Now we can apply Markov's Inequality
      \begin{equation}
        \label{eq:7-markov-presol}
        P(X>8000) = \frac{E[X]}{8000}
      \end{equation}
      Where $E[X]$ is the expected number of bulbs out of the 10000
      bulbs which will survive past 60 days. Since the bulbs'
      expectancies are independent,
      \begin{equation}
        \label{eq:7-markov-e}
        E[X] = np = 10000 e^{-\frac{60}{50}})
      \end{equation}
      So that
      \begin{equation}
        \label{eq:7-markov-sol}
        \begin{aligned}
          P(X>8000) &= \frac{10000 e^{-\frac{60}{50}}}{8000} \\
          \implies P(X>8000) &= 0.3764927649 \\
        \end{aligned}
      \end{equation}
    \end{solution}
  \end{Ex}

\end{enumerate}
\end{document}
